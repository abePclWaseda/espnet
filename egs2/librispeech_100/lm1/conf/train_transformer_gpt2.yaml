# This configuration requires Tesla V100-SXM2(32GB) x 16 GPUs It takes about 2 days.
use_amp: true
lm: transformer_opt
lm_conf:
    opt_name: openai-community/gpt2
    remove_head: false

# optimization related
grad_clip: 5.0
batch_type: numel
# batch_bins: 500000000
# accum_grad: 2
batch_bins: 15625000
accum_grad: 64
max_epoch: 25

optim: adam
optim_conf:
   lr: 0.0003
   weight_decay: 1.0e-06
scheduler: warmuplr
scheduler_conf:
   warmup_steps: 2500

best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10  # 10 is good.